Tom Mitchell, April 2011 
Value Iteration for learning V* : assumes P(St+1|St, A) known 
Initialize V(s) to 0 
For t=1, 2, … [Loop until policy good enough] 
   Loop for s in S 
Loop for a in A 
•    
  
   End loop 
End loop 
V(s) converges to V*(s) 
Dynamic programming 
[optimal value can get in zero steps] 
Inductively,  if V is optimal discounted  reward can get in t-1 steps, 
Q(s,a) is value of performing action a from state s and then being 
optimal from then on for the next t-1 steps.  
Optimal expected discounted reward can 
get by taking an action and then being 
optimal for t-1 steps= optimal expected 
discounted reward can get in t steps. 
