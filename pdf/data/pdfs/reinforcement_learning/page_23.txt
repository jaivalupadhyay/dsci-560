Tom Mitchell, April 2011 
[simplicity assume the transitions and rewards are deterministic. ] 
Optimal value of a state s is the 
maximum, over actions aâ€™ of Q(s,aâ€™).  
Given current approx  ğ‘„  to Q, if we are 
in state s and perform action a and get 
to state sâ€™, update our estimate ğ‘„ (ğ‘ , ğ‘) 
to the reward r we got plus gamma 
times the maximum over aâ€™ of ğ‘„ (ğ‘ â€², ğ‘â€²)  
