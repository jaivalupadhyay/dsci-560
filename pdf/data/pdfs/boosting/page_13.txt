Informal Description Adaboost 
â€¢
For t=1,2, â€¦ ,T 
 
â€¢
Construct Dt on {x1, â€¦, xm} 
â€¢
Run A on Dt producing ht: ğ‘‹â†’{âˆ’1,1} (weak classifier) 
xi âˆˆğ‘‹, ğ‘¦ğ‘–âˆˆğ‘Œ= {âˆ’1,1} 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
+ 
- 
- 
- 
- 
- 
- 
- 
- 
ht 
â€¢ Boosting: turns a weak algo into a strong (PAC) learner. 
â€¢
Output Hfinal ğ‘¥= sign  
ğ›¼ğ‘¡â„ğ‘¡ğ‘¥
ğ‘¡=1
 
 
Input: S={(x1, ğ‘¦1), â€¦,(xm, ğ‘¦m)};  
Roughly speaking Dt+1 increases weight on xi if ht incorrect on xi ; 
decreases it on xi if ht  correct. 
weak learning algo A (e.g., NaÃ¯ve Bayes, decision stumps) 
Ïµt = Pxi ~Dt(ht xi â‰ yi) error of ht over Dt 
