â€¢
Goal:  h has small error over D.
â€¢
Algo sees training sample S: (x1,c*(x1)),â€¦, (xm,c*(xm)), xi i.i.d. from D
Training error: ğ‘’ğ‘Ÿğ‘Ÿğ‘†â„= 1
ğ‘š ğ‘–ğ¼â„ğ‘¥ğ‘–â‰ ğ‘âˆ—ğ‘¥ğ‘–
True error: ğ‘’ğ‘Ÿğ‘Ÿğ·â„= Pr
ğ‘¥~ ğ·(â„ğ‘¥â‰ ğ‘âˆ—(ğ‘¥))
â€¢
Does optimization over S, find hypothesis â„âˆˆğ».
PAC/SLT models for Supervised Learning
How often â„ğ‘¥â‰ ğ‘âˆ—(ğ‘¥) over future 
instances drawn at random from D 
â€¢ But, can only measure:
How often â„ğ‘¥â‰ ğ‘âˆ—(ğ‘¥) over training 
instances
Sample complexity: bound ğ‘’ğ‘Ÿğ‘Ÿğ·â„
in terms of ğ‘’ğ‘Ÿğ‘Ÿğ‘†â„
