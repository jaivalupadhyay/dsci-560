â€¢
Goal:  h has small error over D. 
â€¢
Algo sees training sample S: (x1,c*(x1)),â€¦, (xm,c*(xm)), xi i.i.d. from D 
Training error: errS h = 1
m  I h xi â‰ câˆ—xi
i
 
True error: errD h = Pr
x~ D(h x â‰ câˆ—(x)) 
â€¢   Does optimization over S, find hypothesis â„âˆˆğ». 
PAC/SLT models for Supervised Learning 
How often â„ğ‘¥â‰ ğ‘âˆ—(ğ‘¥) over future 
instances drawn at random from D  
â€¢ But, can only measure: 
How often â„ğ‘¥â‰ ğ‘âˆ—(ğ‘¥) over training 
instances 
Sample complexity: bound ğ‘’ğ‘Ÿğ‘Ÿğ·â„ in terms of ğ‘’ğ‘Ÿğ‘Ÿğ‘†â„ 
