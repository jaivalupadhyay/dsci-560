What you should know: 
•  Logistic regression 
–  Functional form follows from Naïve Bayes assumptions 
•  For Gaussian Naïve Bayes assuming variance σi,k = σi 
•  For discrete-valued Naïve Bayes too 
–  But training procedure picks parameters without the 
conditional independence assumption 
–  MCLE training: pick W to maximize P(Y | X, W) 
–  MAP training: pick W to maximize P(W | X,Y) 
•  regularization:   e.g., P(W)  ~ N(0,σ) 
•  helps reduce overfitting  
•  Gradient ascent/descent 
–  General approach when closed-form solutions for MLE, MAP are 
unavailable 
•  Generative vs. Discriminative classifiers 
–  Bias vs. variance tradeoff 
