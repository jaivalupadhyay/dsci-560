# -*- coding: utf-8 -*-
"""LAB4 FINAL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFghwQuW9ZWJL5ZLcMkrE0W05ZlOtUJ3
"""

!pip install numpy pandas matplotlib statsmodels scikit-learn torch torchvision

import pandas as pd

# Load dataset
df = pd.read_csv("processed_stock_data.csv")

# Convert date column to datetime format
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values(by='date')

# Compute Moving Averages
df['SMA_50'] = df['close_price'].rolling(window=50).mean()
df['SMA_200'] = df['close_price'].rolling(window=200).mean()

# Generate Buy/Sell Signals
df['Signal'] = df.apply(lambda row: 'Buy' if row['SMA_50'] > row['SMA_200'] else 'Sell', axis=1)

# Display results
df[['date', 'ticker', 'close_price', 'SMA_50', 'SMA_200', 'Signal']].dropna().head(10)

import numpy as np

# RSI Calculation Function
def compute_rsi(data, window=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df['RSI'] = compute_rsi(df['close_price'])
df['RSI_Signal'] = np.where(df['RSI'] < 30, 'Buy', np.where(df['RSI'] > 70, 'Sell', 'Hold'))

# Display results
df[['date', 'ticker', 'close_price', 'RSI', 'RSI_Signal']].dropna().head(10)

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd

# Load and preprocess the dataset
df = pd.read_csv("processed_stock_data.csv")

# Convert date to datetime and set as index
df['date'] = pd.to_datetime(df['date'])
df = df[df['ticker'] == 'AAPL'].set_index('date')

# Ensure the index has a proper frequency (Business Days)
df = df.asfreq('B')

# Drop NaN values before model training
df['close_price'].fillna(method='ffill', inplace=True)

# Split into train and test sets
train_size = int(len(df) * 0.8)
train, test = df[:train_size], df[train_size:]

# Fit ARIMA model (p=5, d=1, q=0)
model = ARIMA(train['close_price'], order=(5,1,0))
arima_fit = model.fit()

# Generate predictions for test set
test['Predicted'] = arima_fit.forecast(steps=len(test))

# Fill NaNs in predictions (if any)
test['Predicted'].fillna(method='bfill', inplace=True)

# Evaluate performance
mae = mean_absolute_error(test['close_price'], test['Predicted'])
rmse = np.sqrt(mean_squared_error(test['close_price'], test['Predicted']))

print(f"ARIMA MAE: {mae}, RMSE: {rmse}")

# Display results
test[['close_price', 'Predicted']].dropna().head(10)

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import DataLoader, TensorDataset

# Prepare LSTM data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(aapl_df[['close_price']])

# Create sequences for LSTM
def create_sequences(data, seq_length=50):
    sequences, targets = [], []
    for i in range(len(data) - seq_length):
        sequences.append(data[i:i+seq_length])
        targets.append(data[i+seq_length])
    return np.array(sequences), np.array(targets)

seq_length = 50
X, y = create_sequences(scaled_data, seq_length)

# Split into train/test sets
train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Convert to PyTorch tensors
X_train, y_train = torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)
X_test, y_test = torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)

# Create DataLoader for batching
batch_size = 32
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)

# Define LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        return self.fc(lstm_out[:, -1, :])

# Initialize model, loss function, and optimizer
model = LSTMModel()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train LSTM model
epochs = 20
for epoch in range(epochs):
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = criterion(predictions, batch_y)
        loss.backward()
        optimizer.step()

# Make predictions
with torch.no_grad():
    lstm_predictions = model(X_test).numpy()

# Convert predictions back to original scale
lstm_predictions = scaler.inverse_transform(lstm_predictions)
y_test_original = scaler.inverse_transform(y_test.numpy())

# Evaluate LSTM performance
lstm_mae = mean_absolute_error(y_test_original, lstm_predictions)
lstm_rmse = np.sqrt(mean_squared_error(y_test_original, lstm_predictions))

print(f"LSTM MAE: {lstm_mae}, RMSE: {lstm_rmse}")

# Display results
pd.DataFrame({'Actual': y_test_original.flatten(), 'Predicted': lstm_predictions.flatten()}).head(10)